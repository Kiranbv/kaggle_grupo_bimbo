{"cells":[{"cell_type":"code","source":["# this code implements the grupo bimbo inventory problem. This will be coded in python and run in the databricks cloud. \n# This is a regression problem in which the demand has to be estimated.\n\n# importing some useful packages\nfrom pyspark.sql import Row\nimport os\nimport sys\nimport numpy\nfrom pyspark.sql.types import *\nfrom pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, RidgeRegressionWithSGD,LassoWithSGD\nfrom pyspark.mllib.tree import DecisionTree,RandomForest\n\n\ndef parseddata(line):\n   values = [float(x) for x in line]\n   return LabeledPoint(values[10], values[0:5])\n\ndef parsetest(line):\n   values = [float(x) for x in line]\n   return LabeledPoint(values[0],values[1:6])\n\n\n# reading training and test files\ndf_train = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\").load(\"/FileStore/tables/k4hw4vja1471993246226/train_csv-0a84f.gz\")\ndf_test =  sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\").load(\"/FileStore/tables/k4hw4vja1471993246226/test_csv-19b88.gz\")\n#trainsample = df_train.sample(False,0.0001,234)\n#trainsample.saveAsParquetFile(\"sampledat2.parquet\")\n\ntest_rdd = df_test.rdd\ntrain_rdd = df_train.rdd\n\n# parsing the training data and testing data.\ntrain_data = train_rdd.map(parseddata)\ntrain_data1DF = sqlContext.parquetFile(\"sampledat1.parquet\")\ntrain_data1rdd = train_data1DF.rdd\ntrain_data1 = train_data1rdd.map(parseddata)\ntest_data1 = test_rdd.map(parsetest)\ntest_feats = test_data1.map(lambda p: p.features)\ntest_ids = test_data1.map(lambda p: p.label)\n\n# Build the model\n#model = LinearRegressionWithSGD.train(train_data1,step = .0000000000001,iterations =500)\n#model = RidgeRegressionWithSGD.train(train_data1,step = .0000000000001,regParam=0.0001, miniBatchFraction=.5, intercept=True,validateData=True,iterations=100)\n\nmodel = LassoWithSGD.train(train_data1,step = .0000000000001,regParam=0.01, miniBatchFraction=.5, intercept=True,validateData=True,iterations=100)\n\n#model = DecisionTree.trainRegressor(train_data1,{},impurity='variance', maxDepth=5, maxBins=32)\n\n#model = RandomForest.trainRegressor(train_data1, categoricalFeaturesInfo={},\n #                                   numTrees=3, featureSubsetStrategy=\"auto\",\n #                                   impurity='variance', maxDepth=4, maxBins=32)\n\n# Evaluate the model on training data\n#### decision tree,random forest predictions\n#predictions = model.predict(train_data1.map(lambda x: x.features))\n#values_preds = train_data1.map(lambda lp: lp.label).zip(predictions)\n#values_preds.take(100)\n############\n#values_preds = train_data1.map(lambda p: (p.label, model.predict(p.features)))\n#values_preds.take(10)\n#MSE = values_preds.map(lambda (v, p): (v - p)**2).reduce(lambda x, y: x + y) / values_preds.count()\n#print(\"Mean Squared Error = \" + str(MSE))\n\n\n# evaluating model on the test data\ntype(numpy.float64(0).item())\noutput = test_data1.map(lambda lp: (float(model.predict(lp.features))))\n                      \nfinal_op = test_ids.zip(output)\n\nfinalopdf = sqlContext.createDataFrame(final_op, ['id', 'Demanda_uni_equil'])\n\nfinalopdf.write.format('com.databricks.spark.csv').save(path = \"s3n://AKIAIY3GMVBVW5VDIK7A:LLgydxmcAIFATqw2Nl+1KBtC2Rl+XWglAdCb+fPT@kirangrupo198\")\n\n#finalopdf.saveAsTextFile(output.csv)\n#val header: RDD[String] = sc.parallelize(Array(\"id,Demanda_uni_equil\"))\n#header\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%fs ls /results/output.csv/part-00000\n\n\n\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":3}],"metadata":{"name":"grupo_bimbo","notebookId":2231129720843591},"nbformat":4,"nbformat_minor":0}
